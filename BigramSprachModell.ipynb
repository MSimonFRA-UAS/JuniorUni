{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/1h2mTEtvDA1btk1K/kn2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSimonFRA-UAS/JuniorUni/blob/main/BigramSprachModell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "4MKu0RP1QJQr"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "hwDqg3OeZnUl"
      },
      "outputs": [],
      "source": [
        "class BigramSprachModell(object):\n",
        "    \"\"\"\n",
        "    Eine Klasse, welche ein Bigram Sprachmodell implementiert\n",
        "\n",
        "    Attribute:\n",
        "        bigram_anz: Eine Liste von Dictionaries. Das i-te Dictionary enthaelt die Haeufigkeiten fuer Worte, welche auf das i-te Wort folgen.\n",
        "        D.h., falls an der Stelle i = 42 das Wort \"und\" gespeichert wird, speichert bigram_anz[42] ein Dictionary von Haeufigkeiten, wie etwa {dann: 125, danach: 23. ...}\n",
        "        fuer alle Worte, die je nach \"und\" auftreten.\n",
        "        vorh_wort_anz: Eine Liste von Haeufigkeiten fuer jedes Wort, welches als \"vorheriges\" Wort auftritt.\n",
        "        anz: Eine Liste von Haeufigkeiten jedes Worts im Vokabular.\n",
        "    \"\"\"\n",
        "    def __init__(self, bigram_anz: dict, vorh_wort_anz: dict, anz: dict):\n",
        "        self.bigram_anz = bigram_anz\n",
        "        self.vorh_wort_anz = vorh_wort_anz\n",
        "        self.anz = anz\n",
        "        self.total_anz = sum([anz[wort] for wort in anz.keys()])\n",
        "\n",
        "    def get_vokabular(self):\n",
        "        \"\"\"\n",
        "        :Ausgabe: Das Vokabular des Trainingstexts\n",
        "        \"\"\"\n",
        "        return self.anz.keys()\n",
        "\n",
        "    def get_wkt(self, vorh_wort: str, wort: str) -> float:\n",
        "        \"\"\"\n",
        "        Berechnet die Wahrscheinlichkeit P(naechstes Wort = wort | vorheriges Wort = vorh_wort)\n",
        "        :param vorh_wort: vorheriges Wort\n",
        "        :param wort: das naechste Wort\n",
        "        :Ausgabe: Die bedingte Wahrscheinlichkeit\n",
        "        \"\"\"\n",
        "        anz_nach_vorh_wort = self.bigram_anz[vorh_wort]\n",
        "        if wort in anz_nach_vorh_wort:\n",
        "            naechstes_wort_in_kontext_anz = anz_nach_vorh_wort[wort]\n",
        "        else:\n",
        "            naechstes_wort_in_kontext_anz = 0\n",
        "        return naechstes_wort_in_kontext_anz / self.vorh_wort_anz[vorh_wort]\n",
        "\n",
        "\n",
        "def trainiere_bigram_modell(train_seqs: List[List[str]]) -> BigramSprachModell:\n",
        "    bigram_anz = {}\n",
        "    anz = {}\n",
        "    vorh_wort_anz = {}\n",
        "    for train_seq in train_seqs:\n",
        "        for i in range(1, len(train_seq)):\n",
        "            vorh_wort = train_seq[i-1]\n",
        "            wort = train_seq[i]\n",
        "            if vorh_wort not in bigram_anz:\n",
        "                bigram_anz[vorh_wort] = {}\n",
        "            if wort in bigram_anz[vorh_wort]:\n",
        "                bigram_anz[vorh_wort][wort] += 1\n",
        "            else:\n",
        "                bigram_anz[vorh_wort][wort] = 1\n",
        "            if vorh_wort not in vorh_wort_anz:\n",
        "                vorh_wort_anz[vorh_wort] = 1\n",
        "            else:\n",
        "                vorh_wort_anz[vorh_wort] += 1\n",
        "            if wort not in anz:\n",
        "                anz[wort] = 1\n",
        "            else:\n",
        "                anz[wort] += 1\n",
        "    return BigramSprachModell(bigram_anz, vorh_wort_anz, anz), bigram_anz, vorh_wort_anz, anz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BEGIN_SYMBOL = \"<S>\"\n",
        "END_SYMBOL = \"</S>\"\n",
        "\n",
        "\n",
        "def read_wikitext(path: str) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    Liest a Wikitext file at the given path.\n",
        "    :param path: Pfad des einzulesenden Datensatzes\n",
        "    :Ausgabe: Eine geschachtelte Liste List[List[str]]: Die erste Liste ist eine Liste von Zeilen, die zweite Liste ist eine Liste der Worte (Strings) in dieser Zeile.\n",
        "    \"\"\"\n",
        "    print(\"Trainingsdaten werden eingelesen aus der Datei \" + path)\n",
        "    f = open(path)\n",
        "    lines = []\n",
        "    for line in f:\n",
        "        if len(line.strip()) > 0:\n",
        "            this_line = [BEGIN_SYMBOL]\n",
        "            split_line = line.split(\" \")\n",
        "            for word in split_line:\n",
        "                if len(word.strip()) > 0:\n",
        "                    this_line.append(word.strip())\n",
        "            this_line.append(END_SYMBOL)\n",
        "            lines.append(this_line)\n",
        "    print(\"%i Zeilen wurden eingelesen.\" % len(lines))\n",
        "    return lines"
      ],
      "metadata": {
        "id": "pURR6l00dc0L"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = read_wikitext(\"wiki.valid.tokens\")"
      ],
      "metadata": {
        "id": "9X3JefIWfIt_",
        "outputId": "eb868e04-9710-4feb-ac04-f22520990f9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainingsdaten werden eingelesen aus der Datei wiki.valid.tokens\n",
            "4883 Zeilen wurden eingelesen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train[-1]"
      ],
      "metadata": {
        "id": "f1PlUQBdOZUc",
        "outputId": "c69f0ac0-0628-453c-8fdd-1980c79e0f80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<S>',\n",
              " 'In',\n",
              " 'der',\n",
              " 'Generaldebatte',\n",
              " 'über',\n",
              " 'die',\n",
              " 'Politik',\n",
              " 'seiner',\n",
              " 'Regierung',\n",
              " 'kündigte',\n",
              " 'Mas',\n",
              " 'am',\n",
              " '25.',\n",
              " 'September',\n",
              " '2012',\n",
              " 'im',\n",
              " 'Regionalparlament',\n",
              " 'an',\n",
              " ',',\n",
              " 'Neuwahlen',\n",
              " 'für',\n",
              " 'den',\n",
              " '25.',\n",
              " 'November',\n",
              " '2012',\n",
              " 'anzuberaumen',\n",
              " '.',\n",
              " '</S>']"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm, bigram_anzahl, vorh_wort_anzahl, anzahl = trainiere_bigram_modell(train)"
      ],
      "metadata": {
        "id": "EZncxHFugFaC"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_anzahl['November']"
      ],
      "metadata": {
        "id": "XiI-oIiz8lSD",
        "outputId": "280f0274-36a2-41ee-9757-1f90f209b681",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2009': 3,\n",
              " '1854': 1,\n",
              " '1933': 1,\n",
              " '2016': 1,\n",
              " '1875': 1,\n",
              " '1992': 1,\n",
              " 'wurde': 1,\n",
              " '1962': 3,\n",
              " '1955': 1,\n",
              " '1846': 1,\n",
              " '1968': 1,\n",
              " '2011': 2,\n",
              " '2013': 1,\n",
              " '1706': 1,\n",
              " '1827': 1,\n",
              " '1997': 1,\n",
              " 'statt': 1,\n",
              " '1918': 2,\n",
              " '1936': 1,\n",
              " 'und': 1,\n",
              " '1998': 2,\n",
              " '1924': 1,\n",
              " '2006': 1,\n",
              " '1904': 1,\n",
              " '1848': 1,\n",
              " '1914': 1,\n",
              " '2015': 1,\n",
              " '1912': 1,\n",
              " '2010': 2,\n",
              " '1980': 2,\n",
              " '1981': 1,\n",
              " 'nahezu': 1,\n",
              " '1438': 1,\n",
              " '2000': 1,\n",
              " '1919': 1,\n",
              " '2017': 1,\n",
              " '1946': 1,\n",
              " '1872': 1,\n",
              " '1944': 2,\n",
              " '2012': 3,\n",
              " '2004': 1,\n",
              " '2005': 1,\n",
              " '1925': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vorh_wort_anzahl['November']"
      ],
      "metadata": {
        "id": "_Wr9uv2mPn11",
        "outputId": "1d2f5aa7-07ed-41d1-e99d-ebbc4baa2eea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vorhersage(lm, kontext_wort: str, anz_worte = 5):\n",
        "    \"\"\"\n",
        "    Gibt top anz_worte naechsten Worte nach dem kontext_wort aus\n",
        "    :param lm: Sprachmodell\n",
        "    :param kontext_wort: Kontextwort\n",
        "    :param anz_worte: Laenge der Liste mit Worten mit den hoechsten Wahrscheinlichkeiten\n",
        "    \"\"\"\n",
        "    counter = Counter()\n",
        "    for wort in lm.get_vokabular():\n",
        "        counter[wort] = lm.get_wkt(kontext_wort, wort)\n",
        "    result = \"\"\n",
        "    for (wort, count) in counter.most_common(anz_worte):\n",
        "        result += \"(\" + wort + \", \" + repr(counter[wort]) + \"), \"\n",
        "    print(\"Top 5 Worte und Wahrscheinlichkeiten fuer Wort nach \\\"\" + kontext_wort + \"\\\": \" + result[:-2])"
      ],
      "metadata": {
        "id": "Vxbtg6af1guB"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vorhersage(lm, \"November\")"
      ],
      "metadata": {
        "id": "mmoAvzyH1pSZ",
        "outputId": "654993de-a245-4220-bff9-3f716ddafcc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Worte und Wahrscheinlichkeiten fuer Wort nach \"November\": (2009, 0.05454545454545454), (1962, 0.05454545454545454), (2012, 0.05454545454545454), (2011, 0.03636363636363636), (2010, 0.03636363636363636)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_word(lm, context_word: str):\n",
        "    \"\"\"\n",
        "    :param lm:\n",
        "    :param context_word:\n",
        "    :return: A randomly-sampled word to follow context_word according to the probabilities from lm.get_probability.\n",
        "    Hint: you'll want to use something like\n",
        "    import random\n",
        "    random.uniform(0, 1)\n",
        "    to get a random number, then follow the scheme described in the video for how to turn that random number\n",
        "    into a random word.\n",
        "    \"\"\"\n",
        "    import random\n",
        "    sample = random.uniform(0, 1)\n",
        "    prob_sum = 0\n",
        "    latest_word = \"\"\n",
        "    for word in lm.get_vokabular():\n",
        "        latest_word = word\n",
        "        prob = lm.get_wkt(context_word, word)\n",
        "        prob_sum += prob\n",
        "        if prob_sum > sample:\n",
        "            return word\n",
        "    return latest_word"
      ],
      "metadata": {
        "id": "GRsyygmxCAfo"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_sentence(lm, context_word: str):\n",
        "    \"\"\"\n",
        "    :param lm:\n",
        "    :param context_word: An initial word to seed the sentence with\n",
        "    :return: Up to 10 words as a continuation of context_word by repeatedly sampling the next word\n",
        "    \"\"\"\n",
        "    sentence = [context_word]\n",
        "    for i in range(0, 10):\n",
        "        next_word = sample_word(lm, context_word)\n",
        "        context_word = next_word\n",
        "        sentence.append(next_word)\n",
        "        if next_word == END_SYMBOL:\n",
        "            return sentence\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "2cHYAUvnB7L7"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " print(repr(sample_sentence(lm, \"Uni\")))"
      ],
      "metadata": {
        "id": "ruQNKMDaCY24",
        "outputId": "87c740c5-72e2-470f-d01f-6e40dafd4916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Uni', 'Berklee', 'weltweit', 'Hits', 'auch', 'in', 'der', 'Fahrzeugbeschreibung', 'erwähnt', '.', '</S>']\n"
          ]
        }
      ]
    }
  ]
}