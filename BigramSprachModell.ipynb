{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOed/0dBzZP55YAKb3vGGCP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSimonFRA-UAS/JuniorUni/blob/main/BigramSprachModell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hwDqg3OeZnUl"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "class BigramSprachModell(object):\n",
        "    \"\"\"\n",
        "    Class that implements a bigram language model\n",
        "\n",
        "    Attributes:\n",
        "        bigram_anz: A List of dictionaries. The ith dictionary contains counts for words following word i. So if\n",
        "        i = 67 corresponds to \"of\", bigram_counts[67] stores a dict of counts like {the: 125, my: 23. ...} for all words\n",
        "        that we ever see after \"of\"\n",
        "        prev_word_counts: A List of counts for each word appearing as a \"previous\" word, or \"context\" word.\n",
        "        unigram_counts: A List of counts for words appearing as the \"current\" word. These are the same counts as those\n",
        "        estimated by the UnigramLanguageModel\n",
        "    \"\"\"\n",
        "    def __init__(self, bigram_anz: dict, vorh_wort_anz: dict, unigram_anz: dict):\n",
        "        self.bigram_anz = bigram_anz\n",
        "        self.vorh_wort_anz = vorh_wort_anz\n",
        "        self.unigram_anz = unigram_anz\n",
        "        self.total_unigram_anz = sum([unigram_anz[wort] for wort in unigram_anz.keys()])\n",
        "        self.use_multiplicative = True\n",
        "\n",
        "    def get_vokabular(self):\n",
        "        \"\"\"\n",
        "        :return: A set containing the vocabulary of the\n",
        "        \"\"\"\n",
        "        return self.unigram_anz.keys()\n",
        "\n",
        "    def _get_unigram_wkt(self, wort: str) -> float:\n",
        "        \"\"\"\n",
        "        Helper method to calculate the unigram probability of the given word\n",
        "        :param word: The index of the word to get the unigram probability for\n",
        "        :return: The unigram probability of the word\n",
        "        \"\"\"\n",
        "        return float(self.unigram_anz[wort])/self.total_unigram_anz\n",
        "\n",
        "    def get_wkt(self, vorh_wort: str, wort: str) -> float:\n",
        "        \"\"\"\n",
        "        Computes the probability P(word | prev_word)\n",
        "        :param prev_word: the previous word\n",
        "        :param word: the next word (candidate) to score\n",
        "        :return: The float bigram probability of word given prev_word\n",
        "        \"\"\"\n",
        "        anz_nach_vorh_wort = self.bigram_anz[vorh_wort]\n",
        "        if wort in anz_nach_vorh_wort:\n",
        "            naechstes_wort_in_kontext_anz = anz_nach_vorh_wort[wort]\n",
        "        else:\n",
        "            naechstes_wort_in_kontext_anz = 0\n",
        "        return naechstes_wort_in_kontext_anz / self.vorh_wort_anz[vorh_wort]\n",
        "\n",
        "\n",
        "def estimate_bigram_lm(train_seqs: List[List[str]]) -> BigramSprachModell:\n",
        "    bigram_anz = {}\n",
        "    # The following two have to be different because of the start/end of sequence characters\n",
        "    unigram_anz = {}\n",
        "    vorh_wort_anz = {}\n",
        "    for train_seq in train_seqs:\n",
        "        for i in range(1, len(train_seq)):\n",
        "            vorh_wort = train_seq[i-1]\n",
        "            wort = train_seq[i]\n",
        "            if vorh_wort not in bigram_anz:\n",
        "                bigram_anz[vorh_wort] = {}\n",
        "            if wort in bigram_anz[vorh_wort]:\n",
        "                bigram_anz[vorh_wort][wort] += 1\n",
        "            else:\n",
        "                bigram_anz[vorh_wort][wort] = 1\n",
        "            if vorh_wort not in vorh_wort_anz:\n",
        "                vorh_wort_anz[vorh_wort] = 1\n",
        "            else:\n",
        "                vorh_wort_anz[vorh_wort] += 1\n",
        "            if wort not in unigram_anz:\n",
        "                unigram_anz[wort] = 1\n",
        "            else:\n",
        "                unigram_anz[wort] += 1\n",
        "    return BigramSprachModell(bigram_anz, vorh_wort_anz, unigram_anz)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "BEGIN_SYMBOL = \"<S>\"\n",
        "END_SYMBOL = \"</S>\"\n",
        "\n",
        "\n",
        "def read_wikitext(path: str) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    Reads a Wikitext file at the given path.\n",
        "    :param path: The string path of the file to read\n",
        "    :return: A nested List[List[str]]: The first List is of lines, and the second List is of string words on that line\n",
        "    \"\"\"\n",
        "    print(\"Started reading from file \" + path)\n",
        "    f = open(path)\n",
        "    lines = []\n",
        "    for line in f:\n",
        "        # If it's a non-empty line\n",
        "        if len(line.strip()) > 0:\n",
        "            this_line = [BEGIN_SYMBOL]\n",
        "            split_line = line.split(\" \")\n",
        "            for word in split_line:\n",
        "                if len(word.strip()) > 0:\n",
        "                    this_line.append(word.strip())\n",
        "            this_line.append(END_SYMBOL)\n",
        "            lines.append(this_line)\n",
        "    print(\"Read %i lines\" % len(lines))\n",
        "    return lines"
      ],
      "metadata": {
        "id": "pURR6l00dc0L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "url = 'https://github.com/MSimonFRA-UAS/JuniorUni/blob/main'\n",
        "urllib.request.urlretrieve(url, 'wiki.train.tokens')\n",
        "urllib.request.urlretrieve(url, 'wiki.valid.tokens')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2mGWSNQd7lA",
        "outputId": "30a3621e-16aa-4a99-8e94-059a162d5b3a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('wiki.valid.tokens', <http.client.HTTPMessage at 0x7e3409bf69e0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data():\n",
        "    return (read_wikitext(\"wiki.train.tokens\"), read_wikitext(\"wiki.valid.tokens\"))"
      ],
      "metadata": {
        "id": "9X3JefIWfIt_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train,test) = read_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghaZl0HTfaG_",
        "outputId": "16919eef-9282-4e5a-90d9-7e698e2d332f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started reading from file wiki.train.tokens\n",
            "Read 23767 lines\n",
            "Started reading from file wiki.valid.tokens\n",
            "Read 2461 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fP8_rSFXi6HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm = estimate_bigram_lm(train)"
      ],
      "metadata": {
        "id": "EZncxHFugFaC"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}