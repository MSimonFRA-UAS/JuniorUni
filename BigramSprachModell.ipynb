{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOO4eY816O1CYY6+/MNfcwc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSimonFRA-UAS/JuniorUni/blob/main/BigramSprachModell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random"
      ],
      "metadata": {
        "id": "4MKu0RP1QJQr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hwDqg3OeZnUl"
      },
      "outputs": [],
      "source": [
        "class BigramSprachModell(object):\n",
        "    \"\"\"\n",
        "    Eine Klasse, welche ein Bigram Sprachmodell implementiert\n",
        "\n",
        "    Attribute:\n",
        "        bigram_anz: Eine Liste von Dictionaries. Das i-te Dictionary enthaelt die Haeufigkeiten fuer Worte, welche auf das i-te Wort folgen.\n",
        "        D.h., falls an der Stelle i = 42 das Wort \"und\" gespeichert wird, speichert bigram_anz[42] ein Dictionary von Haeufigkeiten, wie etwa {dann: 125, danach: 23. ...}\n",
        "        fuer alle Worte, die je nach \"und\" auftreten.\n",
        "        vorh_wort_anz: Eine Liste von Haeufigkeiten fuer jedes Wort, welches als \"vorheriges\" Wort auftritt.\n",
        "        anz: Eine Liste von Haeufigkeiten jedes Worts im Vokabular.\n",
        "    \"\"\"\n",
        "    def __init__(self, bigram_anz: dict, vorh_wort_anz: dict, anz: dict):\n",
        "        self.bigram_anz = bigram_anz\n",
        "        self.vorh_wort_anz = vorh_wort_anz\n",
        "        self.anz = anz\n",
        "        self.total_anz = sum([anz[wort] for wort in anz.keys()])\n",
        "\n",
        "    def get_vokabular(self):\n",
        "        \"\"\"\n",
        "        :Ausgabe: Das Vokabular des Trainingstexts\n",
        "        \"\"\"\n",
        "        return self.anz.keys()\n",
        "\n",
        "    def get_wkt(self, vorh_wort: str, wort: str) -> float:\n",
        "        \"\"\"\n",
        "        Berechnet die Wahrscheinlichkeit P(naechstes Wort = wort | vorheriges Wort = vorh_wort)\n",
        "        :param vorh_wort: vorheriges Wort\n",
        "        :param wort: das naechste Wort\n",
        "        :Ausgabe: Die bedingte Wahrscheinlichkeit\n",
        "        \"\"\"\n",
        "        anz_nach_vorh_wort = self.bigram_anz[vorh_wort]\n",
        "        if wort in anz_nach_vorh_wort:\n",
        "            naechstes_wort_in_kontext_anz = anz_nach_vorh_wort[wort]\n",
        "        else:\n",
        "            naechstes_wort_in_kontext_anz = 0\n",
        "        return naechstes_wort_in_kontext_anz / self.vorh_wort_anz[vorh_wort]\n",
        "\n",
        "\n",
        "def trainiere_bigram_modell(train_seqs: List[List[str]]) -> BigramSprachModell:\n",
        "    bigram_anz = {}\n",
        "    anz = {}\n",
        "    vorh_wort_anz = {}\n",
        "    for train_seq in train_seqs:\n",
        "        for i in range(1, len(train_seq)):\n",
        "            vorh_wort = train_seq[i-1]\n",
        "            wort = train_seq[i]\n",
        "            if vorh_wort not in bigram_anz:\n",
        "                bigram_anz[vorh_wort] = {}\n",
        "            if wort in bigram_anz[vorh_wort]:\n",
        "                bigram_anz[vorh_wort][wort] += 1\n",
        "            else:\n",
        "                bigram_anz[vorh_wort][wort] = 1\n",
        "            if vorh_wort not in vorh_wort_anz:\n",
        "                vorh_wort_anz[vorh_wort] = 1\n",
        "            else:\n",
        "                vorh_wort_anz[vorh_wort] += 1\n",
        "            if wort not in anz:\n",
        "                anz[wort] = 1\n",
        "            else:\n",
        "                anz[wort] += 1\n",
        "    return BigramSprachModell(bigram_anz, vorh_wort_anz, anz), bigram_anz, vorh_wort_anz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BEGIN_SYMBOL = \"<S>\"\n",
        "END_SYMBOL = \"</S>\"\n",
        "\n",
        "\n",
        "def read_wikitext(path: str) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    Liest a Wikitext file at the given path.\n",
        "    :param path: Pfad des einzulesenden Datensatzes\n",
        "    :Ausgabe: Eine geschachtelte Liste List[List[str]]: Die erste Liste ist eine Liste von Zeilen, die zweite Liste ist eine Liste der Worte (Strings) in dieser Zeile.\n",
        "    \"\"\"\n",
        "    print(\"Trainingsdaten werden eingelesen aus der Datei \" + path)\n",
        "    f = open(path)\n",
        "    lines = []\n",
        "    for line in f:\n",
        "        if len(line.strip()) > 0:\n",
        "            this_line = [BEGIN_SYMBOL]\n",
        "            split_line = line.split(\" \")\n",
        "            for word in split_line:\n",
        "                if len(word.strip()) > 0:\n",
        "                    this_line.append(word.strip())\n",
        "            this_line.append(END_SYMBOL)\n",
        "            lines.append(this_line)\n",
        "    print(\"%i Zeilen wurden eingelesen.\" % len(lines))\n",
        "    return lines"
      ],
      "metadata": {
        "id": "pURR6l00dc0L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = read_wikitext(\"wiki.valid.tokens\")"
      ],
      "metadata": {
        "id": "9X3JefIWfIt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55d4291-42bc-4240-edea-8ac882eeb59b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainingsdaten werden eingelesen aus der Datei wiki.valid.tokens\n",
            "4883 Zeilen wurden eingelesen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1PlUQBdOZUc",
        "outputId": "07c78dff-6ccd-4cfa-93a0-54e261145ce2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<S>',\n",
              " 'In',\n",
              " 'der',\n",
              " 'Generaldebatte',\n",
              " 'über',\n",
              " 'die',\n",
              " 'Politik',\n",
              " 'seiner',\n",
              " 'Regierung',\n",
              " 'kündigte',\n",
              " 'Mas',\n",
              " 'am',\n",
              " '25.',\n",
              " 'September',\n",
              " '2012',\n",
              " 'im',\n",
              " 'Regionalparlament',\n",
              " 'an',\n",
              " ',',\n",
              " 'Neuwahlen',\n",
              " 'für',\n",
              " 'den',\n",
              " '25.',\n",
              " 'November',\n",
              " '2012',\n",
              " 'anzuberaumen',\n",
              " '.',\n",
              " '</S>']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm, bigram_anzahl, vorh_wort_anzahl = trainiere_bigram_modell(train)"
      ],
      "metadata": {
        "id": "EZncxHFugFaC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_anzahl['November']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiI-oIiz8lSD",
        "outputId": "3016320e-4381-41a8-d971-2568d95c559a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2009': 3,\n",
              " '1854': 1,\n",
              " '1933': 1,\n",
              " '2016': 1,\n",
              " '1875': 1,\n",
              " '1992': 1,\n",
              " 'wurde': 1,\n",
              " '1962': 3,\n",
              " '1955': 1,\n",
              " '1846': 1,\n",
              " '1968': 1,\n",
              " '2011': 2,\n",
              " '2013': 1,\n",
              " '1706': 1,\n",
              " '1827': 1,\n",
              " '1997': 1,\n",
              " 'statt': 1,\n",
              " '1918': 2,\n",
              " '1936': 1,\n",
              " 'und': 1,\n",
              " '1998': 2,\n",
              " '1924': 1,\n",
              " '2006': 1,\n",
              " '1904': 1,\n",
              " '1848': 1,\n",
              " '1914': 1,\n",
              " '2015': 1,\n",
              " '1912': 1,\n",
              " '2010': 2,\n",
              " '1980': 2,\n",
              " '1981': 1,\n",
              " 'nahezu': 1,\n",
              " '1438': 1,\n",
              " '2000': 1,\n",
              " '1919': 1,\n",
              " '2017': 1,\n",
              " '1946': 1,\n",
              " '1872': 1,\n",
              " '1944': 2,\n",
              " '2012': 3,\n",
              " '2004': 1,\n",
              " '2005': 1,\n",
              " '1925': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vorh_wort_anzahl['November']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wr9uv2mPn11",
        "outputId": "4b139111-2bdd-4dbf-ad88-688a326fa1c7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vorhersage(lm, kontext_wort: str, anz_worte = 5):\n",
        "    \"\"\"\n",
        "    Gibt top anz_worte naechsten Worte nach dem kontext_wort aus\n",
        "    :param lm: Sprachmodell\n",
        "    :param kontext_wort: Kontextwort\n",
        "    :param anz_worte: Laenge der Liste mit Worten mit den hoechsten Wahrscheinlichkeiten\n",
        "    \"\"\"\n",
        "    counter = Counter()\n",
        "    for wort in lm.get_vokabular():\n",
        "        counter[wort] = lm.get_wkt(kontext_wort, wort)\n",
        "    result = \"\"\n",
        "    for (wort, count) in counter.most_common(anz_worte):\n",
        "        result += \"(\" + wort + \", \" + repr(counter[wort]) + \"), \"\n",
        "    print(\"Top 5 Worte und Wahrscheinlichkeiten fuer Wort nach \\\"\" + kontext_wort + \"\\\": \" + result[:-2])"
      ],
      "metadata": {
        "id": "Vxbtg6af1guB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vorhersage(lm, \"November\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmoAvzyH1pSZ",
        "outputId": "95cc54fe-a555-430e-b103-ce4d800199c5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Worte und Wahrscheinlichkeiten fuer Wort nach \"November\": (2009, 0.05454545454545454), (1962, 0.05454545454545454), (2012, 0.05454545454545454), (2011, 0.03636363636363636), (2010, 0.03636363636363636)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_wort(lm, kontext_wort: str):\n",
        "    \"\"\"\n",
        "    :param lm: Sprachmodell\n",
        "    :param kontext_wort: Kontextwort\n",
        "    :Ausgabe: Ein zufaellig gesampletes Wort, welches dem Kontextwort folgt entsprechend den Wahrscheinlichkeiten von lm.get_wkt.\n",
        "    \"\"\"\n",
        ""
      ],
      "metadata": {
        "id": "GRsyygmxCAfo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_satz(lm, kontext_wort: str):\n",
        "    \"\"\"\n",
        "    :param lm: Sprachmodell\n",
        "    :param kontext_wort: Kontextwort\n",
        "    :Ausgabe: Satz mit bis zu 10 Worten durch sukzessive Anwendung der sample_wort Routine\n",
        "    \"\"\"\n",
        ""
      ],
      "metadata": {
        "id": "2cHYAUvnB7L7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " print(repr(sample_satz(lm, \"November\")))"
      ],
      "metadata": {
        "id": "ruQNKMDaCY24"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}